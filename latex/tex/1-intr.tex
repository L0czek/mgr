\clearpage % Rozdzia≈Çy zaczynamy od nowej strony.
\section{Introduction} \label{chap:intr}

Testing software became more important in recent times as computer programs grew even more sophisticated. Initially, the programmers who had the best knowledge of the target system implemented the tests by hand. To ease out with this task, many methods of software engineering were created to simplify this process. For example, there exists a software development methodology where a programmer should write the tests first, before implementing the functionality. It is called \textit{Test Driven Development} as the tests should define the design. This is beneficial as the new features can be tested while being developed. Unfortunately, this still doesn't ensure that all bugs and vulnerabilities have been found and taken care off. Apart from that, there is no guarantee that the added functionality doesn't corrupt the already existing ones. It is possible, as adding or changing anything in a computer program might add a new execution path that can lead to software failure. Therefore, software should be extensively tested by constantly checking not only single features but also interactions between various components in a computer system. Sadly, as computer programs become more complicated, it is getting harder to come up with new possible edge cases to test. Of course, this process requires extensive understanding of the program's components, which makes this task even harder. At this point, engineers started employing computer programs to generate tests for other computer programs. This process was named \textit{fuzz testing} or \textit{fuzzing} in short and is a vital part of contemporary software testing and uncovering security vulnerabilities. In real life, fuzzing can be compared to walking into a shop and repetitively trying to, for example, buy minus one item or a huge number of them. During this process, the fuzzer constantly watches the shop assistant to see whether the requests are processed without failures. In the context of computers, the process of fuzzing relies on providing specially prepared input to the program and observing the results. Of course, the easiest approach would be to generate the input at random. Unfortunately, this would require a very long time to met even a simple set of conditions that are checking some bytes in the test case. To approach software testing more sensible, a metric telling whether adding new tests explores previously not tested code paths needed to be found. Such tool is handy not only for software engineers, by helping them in assessing if more tests should be added, but also for fully automated solutions described later. One such metric is referred as \textit{coverage} or \textit{code coverage}. This enabled a more efficient way of fuzzing, which utilizes metaheuristic algorithms to speed the process up. Naturally, the code coverage metric can be used as a fitness function to assess how the well the generated test cases explore different code paths. This approach was a big step in the development of \textit{fuzzers} which are today classified as \textit{genetic fuzzers} and are the standard approach to every fuzzing task. 

After fuzzing applications was well understood, engineers proceeded with developing mechanisms which would allow for targeting operating systems. Changing from testing simple computer programs to operating systems required solving some challenges which were the result of the differences in architecture between a program and an operating system. First, a program is an active entity which relies on the operating system to accomplish its tasks. As a result, the operating system is a passive entity which mainly responds to events and provides a common interface to computer hardware. Therefore, fuzzing an operating system requires the use of a separate computer or a virtual machine to run the target. Additionally, there isn't any simple way of passing input like, for example, in command line programs. Each operating system provides its interface, which is usually a set of functions allowing programs to run and interact with the environment. For this reason, \textit{fuzzers} designed to handle applications needs to be specially modified to target operating systems. Moreover, if the operating system doesn't implement an interface defined in a commonly known standard, finding a prepared and ready to use solution is very difficult.

\subsection{Motivation}
In recent times, embedded system are becoming more and more wide use in all variety of places. Usually such devices come with custom operating systems, as their resources are hardly ever enough for Linux. Additionally, computer systems which require special security measures are getting equipped with small coprocessors designed exclusively to contain and process sensitive information. Examples of this idea are smartphones whose ARM processors have a special secure mode called \textit{Trustzone} which is responsible for providing security features for Android operating system. Moreover, in the past few years many new programming languages were introduced aimed to replace \textit{C} and \textit{C++}. One of these languages is called \textit{Rust} and was primarily designed to create systems and services. Its main advantage over all else is heavy concentration on security. Although, languages like \textit{C} and \textit{C++} are evolving, they still allow for a lot of memory corruption or thread safety bugs. \textit{Rust} was designed with special rules in mind that make it impossible to write code with memory or threading bugs. Therefore, many companies have started to adopt \textit{Rust} and began creating all kind of solutions. Unfortunately, creating a sophisticated enterprise project in \textit{Rust} has a couple of problems. Firstly, as all young languages \textit{Rust} lacks many libraries which leads to mixed projects, where the core is written in \textit{Rust} and therefore secure, but some dependencies might be written in an insecure language like \textit{C}. Naturally, \textit{Rust} allows for such integration, but this might circumvent all enforced security measures. Secondly, as long as the code uses only \textit{Rust} libraries the projects stays secure, but not all code can be made safe. There are places like device drivers where the code access the hardware directly. The security of such accesses cannot be ensured as it required dereferencing raw pointers to memory. For this reason, even though a project is written in \textit{Rust} it still requires extensive testing.

\subsection{Layout and content of the thesis}
This thesis explores a method of \textit{fuzzing} services written in \textit{Rust} in an embedded operating system. First chapter \ref{chap:intr} describes all technologies which will be used in the next sections. It starts with the basics of \textit{fuzzing}, then explores more advanced genetic \textit{fuzzing}. Next topic focuses around the \textit{QEMU} computer hardware emulator. It explores \textit{QEMU}'s architecture and mechanisms essential for system emulation. In the end, this chapter talks about \textit{Rust} programming language and its important features. Next, chapter \ref{chap:qemu} explores different modification applied to \textit{QEMU} which enable \textit{fuzzing} of operating systems. It also shows communication methods between the \textit{fuzzer} and the emulator. The following chapter \ref{chap:envir} describes the design of test case interpretation mechanism. This is the layer which takes raw bytes provided by genetic \textit{fuzzer} and makes function calls based on it. This chapter starts with brief description showing how the mentioned components fit into the whole solution. Then it gives details of designing the special language created to describe the fuzzer's target and a compiler which transpiles them into \textit{Rust} code. The chapter's end illustrates the process of gathering \textit{fuzzer's} corpus from unit tests in the case of system \textit{fuzzing}. Lastly, chapter \ref{chap:tests} shows and discusses the collected results.